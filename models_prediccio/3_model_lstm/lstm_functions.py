# ===============================================================
# Funcions per a la predicci√≥ de temperatura amb xarxes LSTM
# ===============================================================
# Autor: Pau Rodrigo
# Projecte: TFM - Predicci√≥ de temperatura a curt termini amb LSTM
# Data: Juny 2025
#
# Descripci√≥:
# Aquest fitxer cont√© funcions modulars per a la preparaci√≥ de dades,
# entrenament de models LSTM, prediccions (batch i iteratives) i c√†lcul
# de m√®triques. Est√† pensat per a ser importat des d'un notebook principal.
#
# Llibreries requerides:
# - numpy
# - pandas
# - tensorflow / keras
# - scikit-learn
# - matplotlib (nom√©s si es fan gr√†fics)
#
#
# √ös recomanat:
# from lstm_functions import*
# ===============================================================

# ============================
# Llibreries est√†ndard
# ============================
import os
import random

# ============================
# Llibreries cient√≠fiques
# ============================
import numpy as np
import pandas as pd
import json

# ============================
# Visualitzaci√≥
# ============================
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.ticker import FuncFormatter, MaxNLocator

# ============================
# Machine Learning
# ============================
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# ============================
# TensorFlow / Keras
# ============================
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# ============================
# Altres (nom√©s si uses display() dins funcions)
# ============================
from IPython.display import display




# ================================================================
# Funcions per la preparaci√≥ de dades 
# ================================================================

# Funci√≥ per escalar les dades a milers (graficaci√≥)

def escala_mil(x, pos):
    """
    Funci√≥ per formatar els ticks de l‚Äôeix Y multiplicant per 1000.
    √âs √∫til quan vols mostrar valors petits (com MSE en [0, 0.01]) en una escala m√©s llegible.

    Args:
        x (float): valor del tick original (per exemple, 0.0013).
        pos (int): posici√≥ del tick en l‚Äôeix (0, 1, 2...). No s‚Äôutilitza aqu√≠, per√≤ √©s necessari
                   perqu√® FuncFormatter sempre crida la funci√≥ amb dos arguments.

    Returns:
        str: valor formatat com a string, multiplicat per 1000 i amb 1 decimal (ex: '1.3').
    """
    val = x * 1000
    return f'{val:.1f}'


# Funci√≥ per separar les dades en train, val i test

def split_dades(df_lstm):
    """
    Separa un DataFrame de dades temporals en conjunts de train, validaci√≥ i test,
    basant-se en dates l√≠mit relatives al m√†xim de la columna 'data'.

    Args:
        df_lstm (pd.DataFrame): DataFrame amb una columna 'data' i una columna 'valor'.
        limit_train (int): mesos per definir el l√≠mit del conjunt de train.
        limit_val (int): mesos per definir el l√≠mit del conjunt de validaci√≥.

    Returns:
        df_train, df_val, df_test: DataFrames separats per train, validaci√≥ i test.
    """
    
    # Comprovem que el DataFrame tingui les columnes necess√†ries
    if 'data' not in df_lstm.columns or 'valor' not in df_lstm.columns:
        raise ValueError("El DataFrame ha de tenir les columnes 'data' i 'valor'.")


    # Definim les dates l√≠mit per la separaci√≥
    data_max = df_lstm['data'].max()

    data_limit_train = data_max - pd.DateOffset(months= 6)  # L√≠mit inicial del train
    data_limit_val = data_max - pd.DateOffset(months= 3)    # L√≠mit inicial de la validaci√≥

    # Separem els datasets
    df_train = df_lstm[df_lstm['data'] <= data_limit_train].copy().reset_index(drop=True)
    df_val = df_lstm[(df_lstm['data'] > data_limit_train) & (df_lstm['data'] <= data_limit_val)].copy().reset_index(drop=True)
    df_test = df_lstm[df_lstm['data'] > data_limit_val].copy().reset_index(drop=True)

    return df_train, df_val, df_test



# Crea una funci√≥ per escalar les dades

def escalar_dades(df_train, df_val, df_test, columna='valor', verbose=True):
    """
    Escala els valors d'una columna num√®rica utilitzant MinMaxScaler.
    L'ajust es fa nom√©s sobre el conjunt de train, i despr√©s s'aplica als altres.

    Parameters:
    - df_train, df_val, df_test: DataFrames amb la columna a escalar
    - columna: nom de la columna a escalar (per defecte 'valor')

    Returns:
    - df_train, df_val, df_test: DataFrames amb una nova columna 'columna_scaled'
    - scaler: objecte MinMaxScaler ja entrenat
    """
    
    # Importem el Scaler
    scaler = MinMaxScaler()

    # Escalar nom√©s sobre train i transformar val i test
    df_train[f'{columna}_scaled'] = scaler.fit_transform(df_train[[columna]])
    df_val[f'{columna}_scaled'] = scaler.transform(df_val[[columna]])
    df_test[f'{columna}_scaled'] = scaler.transform(df_test[[columna]])


    # Observem com queden les dades
    print('‚úÖ Escalat completat:')
    print("\n")

    if verbose:

        print('Train dataset shape:', df_train.shape)
        # display(df_train.head())

        print('Validation dataset shape:', df_val.shape)
        # display(df_val.head())

        print('Test dataset shape:', df_test.shape)
        # display(df_test.head())

    return df_train, df_val, df_test, scaler




# Creem una funci√≥ per crear sequences per LSTM d'entrada

def create_sequences(series, window_size, n_outputs=1, n_slide=None, lookahead=0):
    """
    Crea seq√º√®ncies d'entrada i sortida per predicci√≥ simple o multi-output, 
    amb suport per despla√ßament entre finestres (n_slide) i predicci√≥ desfasada (lookahead).

    Args:
        series (array): s√®rie temporal escalada.
        window_size (int): llargada de la finestra d'entrada.
        n_outputs (int): nombre de passos a predir (per defecte 1).
        n_slide (int): quant avancem la finestra a cada iteraci√≥ (per defecte 1).
        lookahead (int): passos entre el final de la finestra i la primera predicci√≥ (per defecte 0).

    LSTM espera una entrada en 3 dimensions:
        (n_samples, window_size, n_features)
        On:
            - n_samples = nombre de finestres que hem generat
            - window_size = longitud de cada finestra (n√∫mero de valors consecutius)
            - n_features = nombre de variables per timestep (en aquest cas, 1 sola: la temperatura)

    Returns:
        X (np.array): seq√º√®ncies d'entrada, forma (samples, window_size, 1).
        y (np.array): seq√º√®ncies de sortida, forma (samples, n_outputs) si n_outputs > 1, 
                      o (samples, 1) si n_outputs = 1.
    """

    if n_slide is None:
        n_slide = n_outputs

    X, y = [], []

    max_i = len(series) - lookahead - n_outputs
    i = window_size

    while i <= max_i:
        seq_x = series[i - window_size:i]
        seq_y = series[i + lookahead : i + lookahead + n_outputs]

        X.append(seq_x)
        if n_outputs == 1:
            y.append(seq_y[0])
        else:
            y.append(seq_y)

        i += n_slide

    X = np.array(X).reshape(-1, window_size, 1)
    y = np.array(y)

    if n_outputs == 1:
        y = y.reshape(-1, 1)

    return X, y




# ================================================================
# Funcions per a la creaci√≥ i entrenament del model LSTM
# ================================================================

def definir_model_lstm(
    window_size,       # mida de la finestra temporal (timesteps)
    n_features,        # nombre de variables d'entrada (features)
    n_outputs,         # nombre de valors a predir
    n_layers=3,        # nombre total de capes LSTM
    n_units=64,        # neurones per cada capa LSTM
    dropout_rate=0.2,  # percentatge de neurones a desactivar (Dropout)
    optimizer='adam',  # optimitzador per compilar el model
    loss='mse'         # funci√≥ de p√®rdua (ideal 'mse' per regressi√≥)
):
    
    """
    Crea i compila un model LSTM seq√ºencial personalitzable.

    Args:
        window_size (int): mida de la finestra temporal (timesteps).
        n_features (int): nombre de variables (features) d'entrada.
        n_outputs (int): nombre de valors a predir.
        n_layers (int): nombre total de capes LSTM.
        n_units (int): neurones per capa LSTM.
        dropout_rate (float): percentatge de neurones a desactivar si s'utilitza Dropout.
        optimizer (str): optimitzador per compilar el model.
        loss (str): funci√≥ de p√®rdua per compilar el model.

    Returns:
        model (keras.Sequential): model LSTM ja compilat.
    """
    model = Sequential()

    # Capa inicial LSTM (amb input_shape)
    # Si hi ha m√©s d'una capa, cal que retorni seq√º√®ncia
    model.add(LSTM(n_units, return_sequences=(n_layers > 1), input_shape=(window_size, n_features)))
    if dropout_rate > 0:
        model.add(Dropout(dropout_rate))

    # Capes interm√®dies (si n_layers >= 3)
    for _ in range(n_layers - 2):
        model.add(LSTM(n_units, return_sequences=True))
        if dropout_rate > 0:
            model.add(Dropout(dropout_rate))

    # √öltima capa LSTM (si n_layers >= 2)(sense return_sequences)
    if n_layers > 1:
        model.add(LSTM(n_units))
        if dropout_rate > 0:
            model.add(Dropout(dropout_rate))

    # Capa de sortida (1 neurona per cada output)
    model.add(Dense(n_outputs))

    # Compilaci√≥
    model.compile(
        optimizer=optimizer,
        loss=loss
        )

    return model



def train_model(
    model,              # model LSTM compilat
    X_train, y_train,   # conjunts d'entrenament
    X_val, y_val,       # conjunts de validaci√≥
    epochs=50,          # nombre d'iteracions d'entrenament (Backpropagation)
    batch_size=32,      # mida del lot, nombre de mostres processades abans de l'actualitzaci√≥ dels pesos.
    patience=5,         # paci√®ncia per l'EarlyStopping. Nombre de epoques sense millora abans d'aturar l'entrenament.
    shuffle=False,      # si es vol barrejar les dades (normalment False en seq√º√®ncies)
    seed=42,            # seed per assegurar la reprodu√Øbilitat
    summary=True       # si es vol mostrar el resum del model al final
):
    """
    Entrena un model LSTM amb validaci√≥ i EarlyStopping, mantenint l'ordre si es vol.

    Args:
        model: model LSTM compilat.
        X_train, y_train: conjunts d'entrenament.
        X_val, y_val: conjunts de validaci√≥.
        epochs (int): nombre d'√®poques.
        batch_size (int): mida del lot.
        patience (int): paci√®ncia per l'EarlyStopping.
        shuffle (bool): si es vol barrejar les dades (normalment False en seq√º√®ncies).
        seed (int): seed per assegurar la reprodu√Øbilitat.

    Returns:
        history: historial de l'entrenament.
    """

    # Fixem llavors per a la reprodu√Øbilitat
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)


    # EarlyStopping per evitar overfitting
    early_stop = EarlyStopping(
        monitor='val_loss',
        patience=patience,
        restore_best_weights=True
    )

    print('Entrenant el model LSTM')

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop],
        shuffle=shuffle,
        verbose=1
    )

    if summary:
        print("\nModel Summary:")
        model.summary()

    return history




def plot_loss_train_val(history,show=True):

    fig, ax = plt.subplots(figsize=(8,4))

    ax.plot(history.history['loss'], label='P√®rdua Entrenament')
    ax.plot(history.history['val_loss'], label='P√®rdua Validaci√≥')
    ax.set_title("Evoluci√≥ de la p√®rdua durant l'entrenament")
    ax.set_xlabel("Epoch")
    ax.set_ylabel("MSE (√ó10‚Åª¬≥)")

    # Per ticks x cada 2 epochs
    ax.set_xticks(np.arange(0, len(history.history['loss']), 1))

    # Format ticks y amb 1 decimal
    ax.yaxis.set_major_formatter(FuncFormatter(escala_mil))

    # For√ßa ticks a enters (per√≤ com fem decimals, pot ser opcional)
    ax.yaxis.set_major_locator(MaxNLocator(integer=False, nbins=6))  # nbins controla nombre m√†xim ticks

    ax.legend()
    ax.grid(True)

    if show:
        plt.show()
    else:
        plt.close(fig)

    return fig


# ================================================================
# Funcions de predicci√≥ LSTM 1 output
# ================================================================

def prediccio_batch(model, X_test, df_test_pred, scaler, nom_columna='pred_batch',lookahead=0):
    """
    Fa una predicci√≥ batch (totes les finestres alhora), desescala les prediccions i les afegeix directament a df_test_pred.

    Args:
        model: model LSTM entrenat.
        X_test (np.array): finestres d‚Äôentrada per a la predicci√≥ (forma: (n_samples, window_size, 1)).
        df_test_pred (pd.DataFrame): DataFrame amb la columna 'valor' desescalada. Es modifica in-place.
        scaler: MinMaxScaler ajustat sobre les dades de train.
        nom_columna (str): nom de la columna on s‚Äôenganxaran les prediccions (per defecte 'pred_batch').

    Returns:
        df_test_pred (DataFrame): DataFrame amb la nova columna de predicci√≥.
        y_pred_rescaled (np.array): prediccions desescalades (forma: (n_samples,)).
    """
    # Predicci√≥ i desescalat
    y_pred = model.predict(X_test, verbose=0)
    y_pred_rescaled = scaler.inverse_transform(y_pred).flatten()

    # Assignar al DataFrame (ignorant les primeres files sense prou context)
    window_size = X_test.shape[1]
    idx_valid = df_test_pred.index[window_size + lookahead:]
    df_test_pred.loc[idx_valid, nom_columna] = y_pred_rescaled

    return df_test_pred



def prediccio_step_iterativa(model, X_test, df_test_pred, scaler, nom_columna='pred_iter', lookahead=0):
    """
    Fa una predicci√≥ multi-step iterativa, reinjectant cada predicci√≥ com a nou input,
    i afegeix les prediccions desescalades directament a df_test_pred.

    Args:
        model: model LSTM entrenat.
        X_test (np.array): finestres d‚Äôentrada (forma: (n_samples, window_size, 1)).
        df_test_pred (pd.DataFrame): DataFrame amb la columna 'valor' desescalada. Es modifica in-place.
        scaler: MinMaxScaler ajustat sobre les dades de train.
        nom_columna (str): nom de la columna on s‚Äôenganxaran les prediccions (per defecte 'pred_iter').

    Returns:
        df_test_pred (DataFrame): amb la nova columna de predicci√≥ iterativa afegida.
        y_pred_rescaled (np.array): prediccions desescalades (forma: (n_samples,)).
    """
    window_size = X_test.shape[1]
    n_passos = X_test.shape[0]

    seq = X_test[0].copy()  # Seq√º√®ncia inicial escalada (window_size, 1)
    preds_scaled = []

    for _ in range(n_passos):
        input_seq = seq.reshape((1, window_size, 1))
        pred_scaled = model.predict(input_seq, verbose=0)[0, 0]
        preds_scaled.append(pred_scaled)
        seq = np.append(seq[1:], [[pred_scaled]], axis=0)

    # Desescalar les prediccions
    y_pred_rescaled = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()

    # Assignar les prediccions desescalades al final del DataFrame
    idx_valid = df_test_pred.index[window_size + lookahead : window_size + lookahead + n_passos]
    df_test_pred.loc[idx_valid, nom_columna] = y_pred_rescaled

    return df_test_pred



def prediccio_iterativa_reinjection(model, X_test, df_test_pred, scaler, reinjeccio=5, nom_columna='pred_reinject', lookahead=0):
    """
    Fa una predicci√≥ iterativa amb reinjecci√≥ de valors reals cada 'reinjeccio' passos,
    i afegeix les prediccions desescalades al df_test_pred.

    Args:
        model: model LSTM entrenat.
        X_test (np.array): finestres d‚Äôentrada (forma: (n_samples, window_size, 1)).
        df_test_pred (pd.DataFrame): DataFrame amb la columna 'valor_scaled'. Es modifica in-place.
        scaler: MinMaxScaler ajustat sobre les dades de train.
        reinjeccio (int): nombre de passos entre reinjeccions de dades reals.
        nom_columna (str): nom de la columna on s‚Äôenganxaran les prediccions.

    Returns:
        df_test_pred (DataFrame): amb la nova columna de prediccions afegida.
        y_pred_rescaled (np.array): prediccions desescalades (forma: (n_samples,)).
    """
    window_size = X_test.shape[1]
    n_passos = X_test.shape[0]

    valors_scaled = df_test_pred['valor_scaled'].values
    preds_scaled = []

    # Inicialitzem amb valors reals escalats
    # seq = X_test[0].copy()  # Seq√º√®ncia inicial (forma: (window_size, 1))
    seq = valors_scaled[:window_size].reshape(-1, 1).copy()
    
    for i in range(n_passos):
        input_seq = seq.reshape((1, window_size, 1))
        pred_scaled = model.predict(input_seq, verbose=0)[0, 0]
        preds_scaled.append(pred_scaled)

        if (i + 1) % reinjeccio == 0:
            start_real = i + 1
            end_real = start_real + window_size
            if end_real <= len(valors_scaled):
                seq = valors_scaled[start_real:end_real].reshape(-1, 1).copy()
            else:
                seq = np.append(seq[1:], [[pred_scaled]], axis=0)
        else:
            seq = np.append(seq[1:], [[pred_scaled]], axis=0)


    # Desescalar prediccions
    y_pred_rescaled = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()

    # Assignar al DataFrame
    n_preds = len(preds_scaled)
    idx_valid = df_test_pred.index[window_size + lookahead : window_size + lookahead + n_preds]
    df_test_pred.loc[idx_valid, nom_columna] = y_pred_rescaled

    return df_test_pred




# ================================================================
# Funcions de predicci√≥ LSTM multi-output
# ================================================================


# Funci√≥ per fer prediccions multi-step

def prediccio_batch_multi(model, X_test, df_test, scaler, window_size, n_outputs, nom_columna='pred_batch',lookahead=0):

    """
    Fa prediccions multi-output de manera cont√≠nua i enganxa totes les prediccions al DataFrame original.

    Sup√≤sits:
    - S'utilitzen seq√º√®ncies creades amb `n_slide = n_outputs`, per tant NO hi ha solapament entre finestres.
    - Cada finestra prediu exactament els seg√ºents `n_outputs` valors, i la seg√ºent finestra continua on acaba l‚Äôanterior.

    Args:
        model: Model LSTM multi-output entrenat.
        X_test (np.array): Matriu d‚Äôentrada per a test (n_samples, window_size, 1).
        df_test (pd.DataFrame): DataFrame original amb les dades reals, cont√© almenys la columna 'valor'.
        scaler: MinMaxScaler utilitzat per escalar i desescalar les dades.
        window_size (int): Mida de la finestra d‚Äôentrada per a cada seq√º√®ncia.
        n_outputs (int): Nombre de passos que prediu el model (outputs per finestra).
        nom_columna (str): Nom de la columna on es guardaran les prediccions desescalades.
        lookahead (int): Passos entre el final de la finestra i la primera predicci√≥ (per defecte 0).

    Retorna:
        df_test amb la nova columna `nom_columna` que cont√© les prediccions (amb NaNs on no es pot predir).
    """
    # 1. Fer la predicci√≥ batch
    y_pred = model.predict(X_test, verbose=0)

    # 2. Desescalar les prediccions (per tornar a ¬∞C)

    y_pred_rescaled = scaler.inverse_transform(y_pred)
    y_pred_rescaled = y_pred_rescaled.flatten()  # Aplanar per tenir un vector de prediccions
    
    # 3. Inicialitzem la nova columna amb NaNs
    df_test[nom_columna] = np.nan

    # Calcul rang disponible per a les prediccions
    idx_inici = window_size + lookahead # Inici de les prediccions despr√©s de la finestra inicial
    dispo = len(df_test) - idx_inici  # Espai disponible per a les prediccions
    usable_preds = min(len(y_pred_rescaled), dispo)  # Nombre de prediccions que podem utilitzar

    # 5. Assignaci√≥ segura dels valors
    df_test.iloc[idx_inici:idx_inici + usable_preds, df_test.columns.get_loc(nom_columna)] = y_pred_rescaled[:usable_preds]

    # 6. Av√≠s si hi ha truncament
    if usable_preds < len(y_pred_rescaled):
        print(f"‚ö†Ô∏è {len(y_pred_rescaled) - usable_preds} valors de predicci√≥ no s'han col¬∑locat per falta d'espai. Prediccions truncades a {usable_preds} valors, per que superaven l'espai disponible a df_test.")

    return df_test



def prediccio_step_iterativa_multi(model, X_test, df_test_pred, scaler, nom_columna='pred_iter', lookahead=0):
    """
    Fa una predicci√≥ multi-step iterativa (multi-output), reinjectant les prediccions com a nova entrada,
    i afegeix les prediccions desescalades directament a df_test_pred.

    Assumim que el model prediu diversos passos (multi-output), i que X_test cont√© una sola seq√º√®ncia inicial.

    Args:
        model: model LSTM entrenat.
        X_test (np.array): finestres d‚Äôentrada (forma: (n_samples, window_size, 1)).
        df_test_pred (pd.DataFrame): DataFrame amb la columna 'valor' desescalada. Es modifica in-place.
        scaler: MinMaxScaler ajustat sobre les dades de train.
        nom_columna (str): nom de la columna on s‚Äôenganxaran les prediccions (per defecte 'pred_iter').
        lookahead (int): passos entre el final de la finestra i la primera predicci√≥ (per defecte 0).

    Returns:
        df_test_pred (DataFrame): amb la nova columna de predicci√≥ iterativa afegida.
    """
    window_size = X_test.shape[1]
    n_outputs = model.output_shape[-1]

    seq = X_test[0].copy()  # Seq√º√®ncia inicial escalada (window_size, 1)
    preds_scaled = []

    n_preds_total = len(df_test_pred) - window_size
    n_steps = n_preds_total // n_outputs

    for _ in range(n_steps):
        input_seq = seq.reshape((1, window_size, 1))  # Afegim dimensi√≥ batch
        pred_scaled = model.predict(input_seq, verbose=0)[0]  # (n_outputs,)
        preds_scaled.extend(pred_scaled)

        # Afegim les prediccions escalades al final de la seq√º√®ncia
        pred_scaled_reshaped = pred_scaled.reshape(-1, 1)
        seq = np.concatenate([seq[n_outputs:], pred_scaled_reshaped], axis=0)

    # Desescalar totes les prediccions
    y_pred_rescaled = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()

    # Assignar les prediccions al DataFrame
    idx_valid = df_test_pred.index[window_size + lookahead : window_size + lookahead + len(y_pred_rescaled)]
    df_test_pred.loc[idx_valid, nom_columna] = y_pred_rescaled

    return df_test_pred



def prediccio_iterativa_reinjection_multi(model, X_test, df_test_pred, scaler, reinjeccio=5, nom_columna='pred_reinject',lookahead=0):
    """
    Fa una predicci√≥ iterativa multi-output amb reinjecci√≥ de valors reals cada 'reinjeccio' passos.
    Afegeix les prediccions desescalades al df_test_pred.

    Args:
        model: model LSTM multi-output entrenat.
        X_test (np.array): finestres d‚Äôentrada (forma: (n_samples, window_size, 1)).
        df_test_pred (pd.DataFrame): DataFrame amb la columna 'valor_scaled'. Es modifica in-place.
        scaler: MinMaxScaler ajustat sobre les dades de train.
        reinjeccio (int): cada quants passos es reinjecta el valor real.
        nom_columna (str): nom de la columna on s‚Äôenganxaran les prediccions.

    Returns:
        df_test_pred (DataFrame): amb la nova columna de prediccions afegida.
    """
    window_size = X_test.shape[1]
    n_outputs = model.output_shape[-1]
    valors_scaled = df_test_pred['valor_scaled'].values
    preds_scaled = []

    # Inicialitzem amb la primera finestra real
    seq = valors_scaled[:window_size].reshape(-1, 1).copy()

    # Nombre total de passos de predicci√≥ (amb salt de n_outputs)
    n_preds_total = len(df_test_pred) - window_size
    n_steps = n_preds_total // n_outputs

    for i in range(n_steps):
        input_seq = seq.reshape((1, window_size, 1))
        pred_scaled = model.predict(input_seq, verbose=0)[0]  # (n_outputs,)
        preds_scaled.extend(pred_scaled)

        # Reinjecci√≥ cada 'reinjeccio' passos
        if (i + 1) % reinjeccio == 0:
            start_real = i * n_outputs
            end_real = start_real + window_size
            if end_real <= len(valors_scaled):
                seq = valors_scaled[start_real:end_real].reshape(-1, 1).copy()
            else:
                pred_reshaped = pred_scaled.reshape(-1, 1)
                seq = np.concatenate([seq[n_outputs:], pred_reshaped], axis=0)
        else:
            pred_reshaped = pred_scaled.reshape(-1, 1)
            seq = np.concatenate([seq[n_outputs:], pred_reshaped], axis=0)

    # Desescalar i inserir al DataFrame
    y_pred_rescaled = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()
    idx_valid = df_test_pred.index[window_size + lookahead : window_size + lookahead + len(y_pred_rescaled)]
    df_test_pred.loc[idx_valid, nom_columna] = y_pred_rescaled

    return df_test_pred



# ================================================================
# Funcions de c√†lcul de m√®triques i gr√†fics
# ================================================================


def calcular_metriques(df_test_pred, col_real='valor', col_preds=['pred_batch', 'pred_iter', 'pred_reinject']):
    """
    Calcula RMSE, MSE i MAE per diferents columnes de predicci√≥ respecte a una columna real.

    Args:
        df_test_pred (pd.DataFrame): DataFrame amb les columnes de valors reals i prediccions.
        col_real (str): Nom de la columna amb els valors reals.
        col_preds (list): Llista amb noms de les columnes de predicci√≥.
        window_size (int): Mida de la finestra per alinear les dades.

    Returns:
        pd.DataFrame: Taula amb RMSE, MSE i MAE per cada m√®tode de predicci√≥.
    """
    metriques = {'M√®trica': ['RMSE', 'MSE', 'MAE']}

    # Iterem i calculem les m√®triques per cada columna de predicci√≥
    for col in col_preds:

        # Comprovem que la columna de predicci√≥ existeixi
        if col not in df_test_pred.columns:
            print(f"‚ö†Ô∏è Av√≠s: la columna '{col}' no existeix a df_test_pred. Es descarta.")
            continue

        # Eliminem files amb NaNs (pot ser degut a window_size, lookahead, o prediccions incompletes)
        df_valid = df_test_pred[[col_real, col]].dropna()
        y_true = df_valid[col_real].values
        y_pred = df_valid[col].values

        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        metriques[col] = [rmse, mse, mae]

    df_metriques = pd.DataFrame(metriques).set_index('M√®trica')

    return df_metriques.round(4)



def plot_prediccions(
    df_train,
    df_val,
    df_test_pred,
    col_preds=['pred_batch', 'pred_iter', 'pred_reinject'],
    dies_train=0,
    mostrar_val=False,
    title='Temperatura real i predicci√≥ LSTM',
    show=True
):
    """
    Genera una figura amb les dades reals i les prediccions d'un model LSTM per a una estaci√≥ meteorol√≤gica.

    Aquesta funci√≥ permet representar les dades reals de la s√®rie temporal de temperatura (train, validaci√≥ i test),
    aix√≠ com les prediccions generades pel model LSTM, amb colors fixos per cada estrat√®gia de predicci√≥ per tal de mantenir la consist√®ncia visual.

    Args:
        df_train (pd.DataFrame): DataFrame amb les dades d'entrenament. Ha de contenir com a m√≠nim ['data', 'valor'].
        df_val (pd.DataFrame): DataFrame amb les dades de validaci√≥. Ha de contenir ['data', 'valor'].
        df_test_pred (pd.DataFrame): DataFrame amb les dades de test i prediccions. Ha de contenir ['data', 'valor'] i les columnes de predicci√≥.
        col_preds (list of str): Llista de noms de columnes de predicci√≥ a representar, com ara ['prediccio_batch', 'prediccio_iter'].
        dies_train (int): Nombre de dies finals del train que es volen mostrar (nom√©s si mostrar_train=True).
        mostrar_val (bool): Si es vol representar la s√®rie de validaci√≥.
        title (str): T√≠tol del gr√†fic.
        station (str): Nom de l'estaci√≥ per afegir al t√≠tol.
        show (bool): Si es vol mostrar el gr√†fic al final de la funci√≥. Per defecte True.

    Returns:
        fig (matplotlib.figure.Figure): Objecte figura amb el gr√†fic generat.
    """

    fig, ax = plt.subplots(figsize=(16, 5))

    # Mostrar √∫ltims dies del train i validaci√≥ si s'especifica
    if mostrar_val:
        if dies_train > 0:
            data_limit = df_train['data'].max() - pd.Timedelta(days=dies_train)
            df_train_filtrat = df_train[df_train['data'] >= data_limit]
            ax.plot(df_train_filtrat['data'], df_train_filtrat['valor'], label=f'Train (√∫ltims {dies_train} dies)', color='firebrick', linewidth=1.5)

        ax.plot(df_val['data'], df_val['valor'], label='Validaci√≥', color='darkgreen', linewidth=1.5)


    # Test (color fix)
    ax.plot(df_test_pred['data'], df_test_pred['valor'],
            label='Test', color='steelblue', linewidth=1.5)

    # Colors fixos per a cada estrat√®gia coneguda
    colors_pred = {
        'pred_batch': 'darkorange',
        'pred_iter': 'purple',
        'pred_reinject': 'green'
    }

    linestyle_pred = '--'

    # Prediccions
    if col_preds:
        
        for col in col_preds:
            
            if col not in df_test_pred.columns:
                continue  # Si la columna no existeix, la saltem
            
            color = colors_pred.get(col, 'gray')  # Color per defecte si no est√† definit
            label = col.replace('_', ' ').capitalize()
            ax.plot(df_test_pred['data'], df_test_pred[col],
                    label=label, color=color, linestyle=linestyle_pred, linewidth=1.5)


    # Format general del gr√†fic
    ax.set_title(f'{title}', fontsize=17, weight='bold')
    ax.set_xlabel('Data', fontsize=14)
    ax.set_ylabel('Temperatura (¬∞C)', fontsize=14)
    ax.tick_params(axis='both', labelsize=12)
    ax.grid(True, linestyle='--', alpha=0.5, linewidth=0.6)
    ax.legend(fontsize=12, frameon=False)

    # Format de les dates a l'eix X
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y %m'))
    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
    fig.autofmt_xdate()
    fig.tight_layout()

    # Mostrar el gr√†fic si s'indica
    if show:
        plt.show()
    else:
        plt.close(fig)

    return fig




# ============================================================================================
# Funcions unificades per a la creaci√≥ i entrenament del model LSTM i prediccions
# Aquestes funcions encapsulen tot el proc√©s de preparaci√≥ de dades, entrenament i predicci√≥
# =============================================================================================



# Funci√≥ principal per Definir i entrenar el model LSTM

def deftrain_model_lstm(
    df_lstm,                 # DataFrame amb la columna 'valor' a escalar i utilitzar
    window_size=24,          # Mida de la finestra temporal (timesteps)
    n_outputs=1,             # Nombre de passos a predir (1 per regressi√≥ simple, >1 per multi-output)
    lookahead=0,             # Passos entre el final de la finestra i la primera predicci√≥ (per defecte 0)
    n_layers=3,              # Nombre de capes LSTM
    n_units=64,              # Nombre de neurones per capa LSTM
    dropout_rate=0.2,        # Percentatge de dropout entre capes
    optimizer='adam',        # Optimitzador per compilar el model
    loss='mse',              # Funci√≥ de p√®rdua per compilar el model
    epochs=50,               # Nombre d'√®poques d'entrenament
    batch_size=32,           # Mida del lot per l'entrenament
    patience=5,              # Paciencia per EarlyStopping
    shuffle=False,           # Si es barregen les dades durant l'entrenament (sempre False en seq√º√®ncies temporals)
    seed=42,                 # Llavor per a la reprodu√Øbilitat (per fixar llavors aleat√≤ries en numpy i tensorflow)
    summary=True,            # Si es vol mostrar el resum del model al final de l'entrenament
    show=True                # Si es vol mostrar el gr√†fic de p√®rdua d'entrenament i validaci√≥
):
    """
    Entrena un model LSTM amb les dades proporcionades, aplicant escalat i creaci√≥ de seq√º√®ncies.

    Args:
        df_lstm (pd.DataFrame): DataFrame amb la columna 'valor'.
        window_size (int): Mida de la finestra temporal.
        n_outputs (int): Nombre de passos a predir.
        n_layers (int): Nombre de capes LSTM.
        n_units (int): Nombre de neurones per capa.
        dropout_rate (float): Percentatge de dropout entre capes.
        optimizer (str): Optimitzador.
        loss (str): Funci√≥ de p√®rdua.
        epochs (int): Nombre d‚Äô√®poques d‚Äôentrenament.
        batch_size (int): Mida del lot.
        patience (int): Paciencia per EarlyStopping.
        shuffle (bool): Si es barregen les dades.
        seed (int): Sement per a la reprodu√Øbilitat.

    Returns:
        model, scaler, X_train, y_train, X_val, y_val, X_test, y_test,
        df_train, df_val, df_test, history
    """
    # Separar el DataFrame en train, val i test
    df_train, df_val, df_test = split_dades(df_lstm)

    # Escalar les dades
    df_train, df_val, df_test, scaler = escalar_dades(df_train, df_val, df_test)


    # Crear seq√º√®ncies per la LSTM
    X_train, y_train = create_sequences(df_train['valor_scaled'].values, window_size=window_size, n_outputs=n_outputs, lookahead=lookahead, n_slide=n_outputs)
    X_val, y_val = create_sequences(df_val['valor_scaled'].values, window_size=window_size, n_outputs=n_outputs, lookahead=lookahead ,n_slide=n_outputs)
    X_test, y_test = create_sequences(df_test['valor_scaled'].values, window_size=window_size, n_outputs=n_outputs, lookahead=lookahead, n_slide=n_outputs)



    # Definir i compilar el model LSTM
    model = definir_model_lstm(window_size, n_features=1, n_outputs=n_outputs,
                               n_layers=n_layers, n_units=n_units,
                               dropout_rate=dropout_rate,
                               optimizer=optimizer, loss=loss)


    # Entrenar el model
    history = train_model(model, X_train, y_train, X_val, y_val,
                          epochs=epochs, batch_size=batch_size,
                          patience=patience, shuffle=shuffle, seed=seed,summary=summary)



    # Mostrar Gr√†fic de p√®rdua d'entrenament i validaci√≥
    fig_loss_train = plot_loss_train_val(history, show=show)


    # Retornar els objectes claus del proc√©s
    print('Entrenament completat.')
    return model, scaler, X_train, y_train, X_val, y_val, X_test, y_test, df_train, df_val, df_test, history, fig_loss_train



# Funci√≥ per aplicar prediccions amb un model LSTM entrenat

def prediu_model_lstm(
    model,                                                      # Model LSTM entrenat
    X_test,                                                     # Seq√º√®ncies d‚Äôentrada per a test (forma: (n_samples, window_size, 1))
    df_test,                                                    # DataFrame original de test amb la columna 'valor_scaled'
    scaler,                                                     # MinMaxScaler utilitzat per desescalar les prediccions
    window_size,                                                # Mida de la finestra temporal
    n_outputs ,                                                 # Nombre de passos de predicci√≥ (1 per regressi√≥ simple, >1 per multi-output)
    lookahead=0,                                               # Passos entre el final de la finestra i la primera predicci√≥ (per defecte 0)
    met_pred = ['pred_batch', 'pred_iter', 'pred_reinject']     # Metodes de predicci√≥ a utilitzar
):
    """
    Aplica prediccions amb un model LSTM entrenat.

    Args:
        model (keras.Model): Model LSTM entrenat.
        X_test (np.array): Seq√º√®ncies d‚Äôentrada per a test.
        df_test (pd.DataFrame): DataFrame original de test.
        scaler (MinMaxScaler): Escalador utilitzat per desescalar.
        window_size (int): Mida de finestra temporal.
        n_outputs (int): Nombre de passos de predicci√≥.
        lookahead (int): Passos entre el final de la finestra i la primera predicci√≥.
        met_pred (list): Llista de m√®todes de predicci√≥ a utilitzar. Per defecte, inclou 'pred_batch', 'pred_iter' i 'pred_reinject'.

    Returns:
        df_test_pred (pd.DataFrame): Test amb prediccions.
        metriques (dict): Diccionari amb m√®triques d‚Äôerror.
    """


    # Crear copia df_test per a les prediccions 
    df_test_pred = df_test.copy()


    if n_outputs == 1:

        if 'pred_batch' in met_pred:
            print("Fent predicci√≥ batch...")
            df_test_pred = prediccio_batch(model, X_test, df_test_pred, scaler,lookahead=lookahead)
        
        if 'pred_iter' in met_pred:
            print("Fent predicci√≥ iterativa...")
            df_test_pred = prediccio_step_iterativa(model, X_test, df_test_pred, scaler,lookahead=lookahead)

        if 'pred_reinject' in met_pred:
            print("Fent predicci√≥ iterativa amb reinjecci√≥...")
            df_test_pred = prediccio_iterativa_reinjection(model, X_test, df_test_pred, scaler,lookahead=lookahead)


    else:
        print("Fent predicci√≥ batch multi-output...")
        df_test_pred = prediccio_batch_multi(model, X_test, df_test_pred, scaler,
                                             window_size=window_size, 
                                             n_outputs=n_outputs,
                                             lookahead=lookahead)

        
   
    # Calcular m√®triques per a les prediccions
    metriques = calcular_metriques(df_test_pred, col_real='valor',
                                    col_preds=met_pred)
        

    # Retornar el DataFrame de test amb les prediccions i el dataframe de m√®triques
    return df_test_pred, metriques




# ============================================================================================================
# Pipeline que unifica les funcions de creaci√≥, entrenament i predicci√≥, amb opcions de plot i guardat
# ============================================================================================================

def pipeline_lstm(
    df_lstm,
    window_size=24,
    n_outputs=1,
    lookahead=0,
    n_layers=3,
    n_units=64,
    dropout_rate=0.2,
    optimizer='adam',
    loss='mse',
    epochs=50,
    batch_size=32,
    patience=5,
    shuffle=False,
    seed=42,
    dies_train=0,
    mostrar_val=False,
    col_preds=['pred_batch', 'pred_iter', 'pred_reinject'],
    save_path=None,
    show=True,
    summary=True
):
    """
    Pipeline complet per entrenar, predir i visualitzar un model LSTM de predicci√≥ de temperatura.

    Aquesta funci√≥ integra els tres blocs principals del flux de treball amb xarxes LSTM:
    - Entrenament del model amb dades seq√ºencials
    - Predicci√≥ sobre el conjunt de test
    - Visualitzaci√≥ de les prediccions juntament amb les dades reals

    Si s'indica un directori a `save_path`, es guardaran autom√†ticament:
        - El model entrenat (`model.h5`)
        - L‚Äôhistorial d'entrenament (`loss_history.csv`) i la gr√†fica de p√®rdua (`loss_plot.png`)
        - Les prediccions (`prediccions.csv`)
        - Les m√®triques d‚Äôavaluaci√≥ (`metrics.csv` i `metrics.txt`)
        - La configuraci√≥ de l‚Äôexperiment (`config.json`)

    Retorna:
        model, scaler, df_train, df_val, df_test_pred, history, metriques, fig, fig_loss_train
    """

    print("üß† [1/5] Entrenant el model LSTM...")

    model, scaler, X_train, y_train, X_val, y_val, X_test, y_test, df_train, df_val, df_test, history, fig_loss_train = deftrain_model_lstm(
        df_lstm=df_lstm,
        window_size=window_size,
        n_outputs=n_outputs,
        lookahead=lookahead,
        n_layers=n_layers,
        n_units=n_units,
        dropout_rate=dropout_rate,
        optimizer=optimizer,
        loss=loss,
        epochs=epochs,
        batch_size=batch_size,
        patience=patience,
        shuffle=shuffle,
        seed=seed,
        summary= summary,
        show=show
    )

    if save_path:
        print("üíæ [2/5] Guardant model i gr√†fica de p√®rdua...")
        os.makedirs(save_path, exist_ok=True)
        model.save(os.path.join(save_path, "model.h5"), include_optimizer=False)
        pd.DataFrame(history.history).to_csv(os.path.join(save_path, "loss_history.csv"))
        fig_loss_train.savefig(os.path.join(save_path, "loss_plot.png"))

    else:
        print("üìÇ [2/5] Model entrenat")

    print("üîÆ [3/5] Fent prediccions...")

    df_test_pred, metriques = prediu_model_lstm(
        model=model,
        X_test=X_test,
        df_test=df_test,
        scaler=scaler,
        window_size=window_size,
        n_outputs=n_outputs,
        lookahead=lookahead,
        met_pred=col_preds
    )

    print("üìä [4/5] Generant gr√†fic de prediccions...")

    fig = plot_prediccions(
        df_train=df_train,
        df_val=df_val,
        df_test_pred=df_test_pred,
        col_preds=col_preds,
        dies_train=dies_train,
        mostrar_val=mostrar_val,
        title='Temperatura real i predicci√≥ LSTM',
        show=show
    )

    # Mostrar les metriques calculades
    print("\nüìà M√®triques calculades:")
    print(metriques)


    if save_path:
        print("üóÉÔ∏è [5/5] Guardant prediccions, m√®triques i configuraci√≥...")
        
        df_test_pred.to_csv(os.path.join(save_path, "prediccions.csv"), index=False)
        metriques.to_csv(os.path.join(save_path, "metrics.csv"))

        # Guardar les m√®triques com si fos un CSV per√≤ en fitxer .txt
        metriques.to_csv(os.path.join(save_path, "metrics.txt"))


        fig.savefig(os.path.join(save_path, "plot.png"))

        config = {
            'window_size': window_size,
            'n_outputs': n_outputs,
            'lookahead': lookahead,
            'n_layers': n_layers,
            'n_units': n_units,
            'dropout_rate': dropout_rate,
            'optimizer': optimizer,
            'loss': loss,
            'epochs': epochs,
            'batch_size': batch_size,
            'patience': patience,
            'shuffle': shuffle,
            'seed': seed
        }

        with open(os.path.join(save_path, "config.json"), "w") as f:
            json.dump(config, f, indent=2)

    else:
        print("üìÇ [5/5] Pipeline completada sense guardar resultats.")

    return model, scaler, df_train, df_val, df_test_pred, history, metriques, fig, fig_loss_train




# ===============================================================
# FUNCI√ì PER CONSTRUIR NOM D'EXPERIMENT
# ===============================================================


def construir_nom_experiment(params: dict, prefix="exp"):
    """
    Construeix un nom √∫nic per identificar cada experiment segons els hiperpar√†metres.
    """
    parts = [
        f"win{params.get('window_size', 24)}",
        f"out{params.get('n_outputs', 1)}",
        f"look{params.get('lookahead', 0)}",
        f"lay{params.get('n_layers', 2)}",
        f"uni{params.get('n_units', 64)}",
        f"drop{int(params.get('dropout_rate', 0.2) * 100)}"
    ]
    nom = "_".join(parts)
    return f"{prefix}_{nom}"






# ===============================================================
# WRAPPERS D'ENTRENAMENT I EXECUCI√ì D'EXPERIMENTS
# ===============================================================


def executar_experiment(
    df,
    params: dict,
    save_path: str = None,
    col_preds=['pred_batch', 'pred_iter', 'pred_reinject'],
    dies_train=0,
    mostrar_val=False
):
    """
    Wrapper per executar un experiment amb la pipeline LSTM i guardar-ne els resultats.

    Args:
        df (pd.DataFrame): DataFrame amb les dades originals.
        params (dict): Diccionari amb hiperpar√†metres de l'experiment.
        save_path (str, optional): Ruta per guardar els resultats.
        col_preds (list): Columnes de predicci√≥ a mostrar o guardar.
        dies_train (int): Dies finals de train a mostrar (si escau).
        mostrar_val (bool): Si s'ha de mostrar tamb√© la validaci√≥.
    """


    if save_path is None:
        nom = construir_nom_experiment(params)
        save_path = os.path.join("resultats", nom)

    pipeline_lstm(
        df_lstm=df,
        window_size=params.get('window_size', 24),
        n_outputs=params.get('n_outputs', 1),
        lookahead=params.get('lookahead', 0),
        n_layers=params.get('n_layers', 3),
        n_units=params.get('n_units', 64),
        dropout_rate=params.get('dropout_rate', 0.2),
        optimizer=params.get('optimizer', 'adam'),
        loss=params.get('loss', 'mse'),
        epochs=params.get('epochs', 50),
        batch_size=params.get('batch_size', 32),
        patience=params.get('patience', 5),
        shuffle=params.get('shuffle', False),
        seed=params.get('seed', 42),
        dies_train=dies_train,
        mostrar_val=mostrar_val,
        col_preds=col_preds,
        save_path=save_path,
        summary=False,         # ‚ùå Desactiva el resum del model al fer experiments massius
        show=False             # ‚ùå Desactiva el plot interactiu al fer experiments massius
    )































